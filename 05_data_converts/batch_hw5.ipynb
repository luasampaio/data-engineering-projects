{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1262611c-ab02-4086-858c-54a04d7e8583",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/10 20:27:17 WARN Utils: Your hostname, multisrf2 resolves to a loopback address: 127.0.1.1; using 192.168.1.138 instead (on interface wlp47s0)\n",
      "25/03/10 20:27:17 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/03/10 20:27:17 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/03/10 20:27:18 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Version: 3.5.5\n"
     ]
    }
   ],
   "source": [
    "# Install PySpark (if not already installed)\n",
    "# !pip install pyspark\n",
    "\n",
    "# Import SparkSession from pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a local Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"Module5Homework\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Print the Spark version\n",
    "print(\"Spark Version:\", spark.version)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ba00346-7ea5-48e7-aa68-3e0bc629dc41",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-03-10 20:27:59--  https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-10.parquet\n",
      "3.160.226.161, 3.160.226.85, 3.160.226.111, ...vzurychx.cloudfront.net)... \n",
      "Connecting to d37ci6vzurychx.cloudfront.net (d37ci6vzurychx.cloudfront.net)|3.160.226.161|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 64346071 (61M) [binary/octet-stream]\n",
      "Saving to: ‘yellow_tripdata_2024-10.parquet’\n",
      "\n",
      "yellow_tripdata_202 100%[===================>]  61.36M  16.6MB/s    in 3.9s    \n",
      "\n",
      "2025-03-10 20:28:03 (15.7 MB/s) - ‘yellow_tripdata_2024-10.parquet’ saved [64346071/64346071]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Download the Parquet data (this command runs in the shell)\n",
    "!wget https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-10.parquet\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31dff9f1-146e-471c-803c-c875f876e8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the Parquet file into a Spark DataFrame\n",
    "df = spark.read.parquet(\"yellow_tripdata_2024-10.parquet\")\n",
    "# Repartition the DataFrame into 4 partitions\n",
    "df_repart = df.repartition(4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c95758db-4842-4528-af11-691f5e202c45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Write the repartitioned DataFrame back to Parquet format\n",
    "df_repart.write.mode(\"overwrite\").parquet(\"yellow_tripdata_2024-10_repart.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab434323-b00a-486a-82d8-1e0ce7fddc9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91M\tyellow_tripdata_2024-10_repart.parquet\n"
     ]
    }
   ],
   "source": [
    "!du -sh yellow_tripdata_2024-10_repart.parquet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9271bd-5e14-4b75-a0a8-008af022a146",
   "metadata": {},
   "outputs": [],
   "source": [
    "## QUESTION 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9da0f3e2-35c1-47a9-8b58-9e34812f04a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n"
     ]
    }
   ],
   "source": [
    "!ls -lh yellow_tripdata_2024-10_repart.parquet/*.parquet | awk '{sum += $5} END {print sum/NR}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e97234f-ed2e-4055-b459-209819c5a348",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Parquet file size: 22.39 MB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "# Directory where the parquet files were written\n",
    "output_dir = \"yellow_tripdata_2024-10_repart.parquet\"\n",
    "\n",
    "# Find all .parquet files (ignore .crc files)\n",
    "parquet_files = glob.glob(os.path.join(output_dir, \"*.parquet\"))\n",
    "\n",
    "# Calculate the total size of the .parquet files\n",
    "total_size_bytes = 0\n",
    "for f in parquet_files:\n",
    "    total_size_bytes += os.path.getsize(f)\n",
    "\n",
    "# Compute average size in MB\n",
    "if len(parquet_files) > 0:\n",
    "    avg_size_mb = total_size_bytes / len(parquet_files) / (1024 * 1024)\n",
    "    print(f\"Average Parquet file size: {avg_size_mb:.2f} MB\")\n",
    "else:\n",
    "    print(\"No .parquet files found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f19136ca-9e7c-4ea8-9a3c-a917f159198c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## question 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "64f09a98-20d9-45a8-b4ae-86f95467cb22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using pickup column: tpep_pickup_datetime\n",
      "Number of trips on October 15th: 128893\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_date, col\n",
    "\n",
    "# Let's assume 'df' is the DataFrame containing your taxi data\n",
    "columns_lower = [c.lower() for c in df.columns]\n",
    "pickup_candidates = [df.columns[i] for i, c in enumerate(columns_lower) if \"pickup\" in c]\n",
    "\n",
    "if len(pickup_candidates) == 0:\n",
    "    raise ValueError(\"No column found with the word 'pickup' in its name.\")\n",
    "elif len(pickup_candidates) > 1:\n",
    "    raise ValueError(f\"Multiple columns found containing 'pickup': {pickup_candidates}\")\n",
    "else:\n",
    "    pickup_col = pickup_candidates[0]\n",
    "    print(f\"Using pickup column: {pickup_col}\")\n",
    "\n",
    "# Now filter for October 15th (2024-10-15)\n",
    "df_oct15 = df.filter(to_date(col(pickup_col)) == \"2024-10-15\")\n",
    "\n",
    "# Count records\n",
    "trip_count_oct15 = df_oct15.count()\n",
    "print(\"Number of trips on October 15th:\", trip_count_oct15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1bb25b22-9f9d-4b0f-a09d-c5caf7c91fbf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['vendorid',\n",
       " 'tpep_pickup_datetime',\n",
       " 'tpep_dropoff_datetime',\n",
       " 'passenger_count',\n",
       " 'trip_distance',\n",
       " 'ratecodeid',\n",
       " 'store_and_fwd_flag',\n",
       " 'pulocationid',\n",
       " 'dolocationid',\n",
       " 'payment_type',\n",
       " 'fare_amount',\n",
       " 'extra',\n",
       " 'mta_tax',\n",
       " 'tip_amount',\n",
       " 'tolls_amount',\n",
       " 'improvement_surcharge',\n",
       " 'total_amount',\n",
       " 'congestion_surcharge',\n",
       " 'airport_fee']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns_lower\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182ecd18-f4bb-4785-8456-bc0cfab62977",
   "metadata": {},
   "outputs": [],
   "source": [
    "#limit to those that finish same day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "117eb946-1874-4e34-86ab-0be1ed463fd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using pickup column: tpep_pickup_datetime\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_date, col\n",
    "\n",
    "# Assume your DataFrame is called df\n",
    "\n",
    "# 1) Dynamically detect the pickup column name by searching for \"pickup\" in df.columns\n",
    "columns_lower = [c.lower() for c in df.columns]\n",
    "pickup_candidates = [df.columns[i] for i, c in enumerate(columns_lower) if \"pickup\" in c]\n",
    "\n",
    "if len(pickup_candidates) == 0:\n",
    "    raise ValueError(\"No column found with the word 'pickup' in its name.\")\n",
    "elif len(pickup_candidates) > 1:\n",
    "    raise ValueError(f\"Multiple columns found containing 'pickup': {pickup_candidates}\")\n",
    "else:\n",
    "    pickup_col = pickup_candidates[0]\n",
    "    print(f\"Using pickup column: {pickup_col}\")\n",
    "\n",
    "# 2) Filter for trips that started AND ended on 2024-10-15\n",
    "df_oct15 = df.filter(\n",
    "    (to_date(col(pickup_col)) == \"2024-10-15\") &\n",
    "    (to_date(col(\"tpep_dropoff_datetime\")) == \"2024-10-15\")\n",
    ")\n",
    "\n",
    "# # 3) Show the filtered records (or count them)\n",
    "# df_oct15.show()  # or df_oct15.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "526ddc08-5f76-48a4-a7c2-d3e5477c0a7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trips on October 15th: 127993\n"
     ]
    }
   ],
   "source": [
    "# Count records\n",
    "trip_count_oct15 = df_oct15.count()\n",
    "print(\"Number of trips on October 15th:\", trip_count_oct15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07fcefee-9d3b-4d32-91d3-56bdbcd78f25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cdc7d3-047c-46cf-bd15-c5357090315d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## question 4 Longest trip in hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "74f739f9-c5ba-4f26-8481-5d42c642567d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longest trip duration (hours): 162.61777777777777\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import unix_timestamp, max as spark_max\n",
    "\n",
    "# Create a new column for trip duration in hours\n",
    "df_duration = df.withColumn(\"duration_hours\", \n",
    "                            (unix_timestamp(\"tpep_dropoff_datetime\") - unix_timestamp(\"tpep_pickup_datetime\")) / 3600)\n",
    "\n",
    "# Find the longest trip (maximum duration)\n",
    "max_duration = df_duration.select(spark_max(\"duration_hours\")).collect()[0][0]\n",
    "print(\"Longest trip duration (hours):\", max_duration)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e52b2f42-98bb-41ab-a5c3-b08e7519a103",
   "metadata": {},
   "outputs": [],
   "source": [
    "## question 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "de6c564c-84e0-4692-a899-bc59308dc157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-03-10 21:01:04--  https://d37ci6vzurychx.cloudfront.net/misc/taxi_zone_lookup.csv\n",
      "3.160.226.111, 3.160.226.228, 3.160.226.85, ...vzurychx.cloudfront.net)... \n",
      "Connecting to d37ci6vzurychx.cloudfront.net (d37ci6vzurychx.cloudfront.net)|3.160.226.111|:443... connected.\n",
      "200 OKequest sent, awaiting response... \n",
      "Length: 12331 (12K) [text/csv]\n",
      "Saving to: ‘taxi_zone_lookup.csv.1’\n",
      "\n",
      "taxi_zone_lookup.cs 100%[===================>]  12.04K  --.-KB/s    in 0.001s  \n",
      "\n",
      "2025-03-10 21:01:05 (15.5 MB/s) - ‘taxi_zone_lookup.csv.1’ saved [12331/12331]\n",
      "\n",
      "+--------------------+----------+\n",
      "|                zone|trip_count|\n",
      "+--------------------+----------+\n",
      "|Governor's Island...|         1|\n",
      "|       Rikers Island|         2|\n",
      "|       Arden Heights|         2|\n",
      "|         Jamaica Bay|         3|\n",
      "| Green-Wood Cemetery|         3|\n",
      "|Charleston/Totten...|         4|\n",
      "|   Rossville/Woodrow|         4|\n",
      "|       West Brighton|         4|\n",
      "|Eltingville/Annad...|         4|\n",
      "|       Port Richmond|         4|\n",
      "|         Great Kills|         6|\n",
      "|        Crotona Park|         6|\n",
      "|Heartland Village...|         7|\n",
      "|     Mariners Harbor|         7|\n",
      "|Saint George/New ...|         9|\n",
      "|             Oakwood|         9|\n",
      "|       Broad Channel|        10|\n",
      "|New Dorp/Midland ...|        10|\n",
      "|         Westerleigh|        12|\n",
      "|     Pelham Bay Park|        12|\n",
      "+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Download the taxi zone lookup CSV file\n",
    "# !wget https://d37ci6vzurychx.cloudfront.net/misc/taxi_zone_lookup.csv\n",
    "\n",
    "# Load the CSV into a Spark DataFrame\n",
    "zone_df = spark.read.csv(\"taxi_zone_lookup.csv\", header=True, inferSchema=True)\n",
    "zone_df.createOrReplaceTempView(\"zones\")\n",
    "\n",
    "# Create or use a temporary view for the Yellow taxi data if not already done\n",
    "df.createOrReplaceTempView(\"yellow\")\n",
    "\n",
    "# Use a Spark SQL query to join the data, count trips per zone, and select the least frequent one.\n",
    "least_frequent_zone = spark.sql(\"\"\"\n",
    "SELECT z.zone, COUNT(*) as trip_count\n",
    "FROM yellow y\n",
    "JOIN zones z ON y.PULocationID = z.LocationID\n",
    "GROUP BY z.zone\n",
    "ORDER BY trip_count ASC\n",
    "\"\"\")\n",
    "#LIMIT 1\n",
    "\n",
    "least_frequent_zone.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62b30da-1db9-4d8a-9896-d48cf6c4317d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d50482-115a-447a-8578-c255fb385d0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b04599-3ab0-4e42-8766-4c6f659f0154",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python_3.10.12(.env_dataeng)",
   "language": "python",
   "name": ".env_dataeng"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
