{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1262611c-ab02-4086-858c-54a04d7e8583",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install PySpark (if not already installed)\n",
    "# !pip install pyspark\n",
    "\n",
    "# Import SparkSession from pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a local Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"Module5Homework\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Print the Spark version\n",
    "print(\"Spark Version:\", spark.version)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba00346-7ea5-48e7-aa68-3e0bc629dc41",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Download the Parquet data (this command runs in the shell)\n",
    "!wget https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-10.parquet\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31dff9f1-146e-471c-803c-c875f876e8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the Parquet file into a Spark DataFrame\n",
    "df = spark.read.parquet(\"yellow_tripdata_2024-10.parquet\")\n",
    "# Repartition the DataFrame into 4 partitions\n",
    "df_repart = df.repartition(4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95758db-4842-4528-af11-691f5e202c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the repartitioned DataFrame back to Parquet format\n",
    "df_repart.write.mode(\"overwrite\").parquet(\"yellow_tripdata_2024-10_repart.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab434323-b00a-486a-82d8-1e0ce7fddc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!du -sh yellow_tripdata_2024-10_repart.parquet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9271bd-5e14-4b75-a0a8-008af022a146",
   "metadata": {},
   "outputs": [],
   "source": [
    "## QUESTION 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da0f3e2-35c1-47a9-8b58-9e34812f04a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lh yellow_tripdata_2024-10_repart.parquet/*.parquet | awk '{sum += $5} END {print sum/NR}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e97234f-ed2e-4055-b459-209819c5a348",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "# Directory where the parquet files were written\n",
    "output_dir = \"yellow_tripdata_2024-10_repart.parquet\"\n",
    "\n",
    "# Find all .parquet files (ignore .crc files)\n",
    "parquet_files = glob.glob(os.path.join(output_dir, \"*.parquet\"))\n",
    "\n",
    "# Calculate the total size of the .parquet files\n",
    "total_size_bytes = 0\n",
    "for f in parquet_files:\n",
    "    total_size_bytes += os.path.getsize(f)\n",
    "\n",
    "# Compute average size in MB\n",
    "if len(parquet_files) > 0:\n",
    "    avg_size_mb = total_size_bytes / len(parquet_files) / (1024 * 1024)\n",
    "    print(f\"Average Parquet file size: {avg_size_mb:.2f} MB\")\n",
    "else:\n",
    "    print(\"No .parquet files found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19136ca-9e7c-4ea8-9a3c-a917f159198c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## question 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f09a98-20d9-45a8-b4ae-86f95467cb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date, col\n",
    "\n",
    "# Let's assume 'df' is the DataFrame containing your taxi data\n",
    "columns_lower = [c.lower() for c in df.columns]\n",
    "pickup_candidates = [df.columns[i] for i, c in enumerate(columns_lower) if \"pickup\" in c]\n",
    "\n",
    "if len(pickup_candidates) == 0:\n",
    "    raise ValueError(\"No column found with the word 'pickup' in its name.\")\n",
    "elif len(pickup_candidates) > 1:\n",
    "    raise ValueError(f\"Multiple columns found containing 'pickup': {pickup_candidates}\")\n",
    "else:\n",
    "    pickup_col = pickup_candidates[0]\n",
    "    print(f\"Using pickup column: {pickup_col}\")\n",
    "\n",
    "# Now filter for October 15th (2024-10-15)\n",
    "df_oct15 = df.filter(to_date(col(pickup_col)) == \"2024-10-15\")\n",
    "\n",
    "# Count records\n",
    "trip_count_oct15 = df_oct15.count()\n",
    "print(\"Number of trips on October 15th:\", trip_count_oct15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb25b22-9f9d-4b0f-a09d-c5caf7c91fbf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "columns_lower\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182ecd18-f4bb-4785-8456-bc0cfab62977",
   "metadata": {},
   "outputs": [],
   "source": [
    "#limit to those that finish same day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117eb946-1874-4e34-86ab-0be1ed463fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date, col\n",
    "\n",
    "# Assume your DataFrame is called df\n",
    "\n",
    "# 1) Dynamically detect the pickup column name by searching for \"pickup\" in df.columns\n",
    "columns_lower = [c.lower() for c in df.columns]\n",
    "pickup_candidates = [df.columns[i] for i, c in enumerate(columns_lower) if \"pickup\" in c]\n",
    "\n",
    "if len(pickup_candidates) == 0:\n",
    "    raise ValueError(\"No column found with the word 'pickup' in its name.\")\n",
    "elif len(pickup_candidates) > 1:\n",
    "    raise ValueError(f\"Multiple columns found containing 'pickup': {pickup_candidates}\")\n",
    "else:\n",
    "    pickup_col = pickup_candidates[0]\n",
    "    print(f\"Using pickup column: {pickup_col}\")\n",
    "\n",
    "# 2) Filter for trips that started AND ended on 2024-10-15\n",
    "df_oct15 = df.filter(\n",
    "    (to_date(col(pickup_col)) == \"2024-10-15\") &\n",
    "    (to_date(col(\"tpep_dropoff_datetime\")) == \"2024-10-15\")\n",
    ")\n",
    "\n",
    "# # 3) Show the filtered records (or count them)\n",
    "# df_oct15.show()  # or df_oct15.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526ddc08-5f76-48a4-a7c2-d3e5477c0a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count records\n",
    "trip_count_oct15 = df_oct15.count()\n",
    "print(\"Number of trips on October 15th:\", trip_count_oct15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07fcefee-9d3b-4d32-91d3-56bdbcd78f25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cdc7d3-047c-46cf-bd15-c5357090315d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## question 4 Longest trip in hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f739f9-c5ba-4f26-8481-5d42c642567d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import unix_timestamp, max as spark_max\n",
    "\n",
    "# Create a new column for trip duration in hours\n",
    "df_duration = df.withColumn(\"duration_hours\", \n",
    "                            (unix_timestamp(\"tpep_dropoff_datetime\") - unix_timestamp(\"tpep_pickup_datetime\")) / 3600)\n",
    "\n",
    "# Find the longest trip (maximum duration)\n",
    "max_duration = df_duration.select(spark_max(\"duration_hours\")).collect()[0][0]\n",
    "print(\"Longest trip duration (hours):\", max_duration)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52b2f42-98bb-41ab-a5c3-b08e7519a103",
   "metadata": {},
   "outputs": [],
   "source": [
    "## question 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6c564c-84e0-4692-a899-bc59308dc157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the taxi zone lookup CSV file\n",
    "# !wget https://d37ci6vzurychx.cloudfront.net/misc/taxi_zone_lookup.csv\n",
    "\n",
    "# Load the CSV into a Spark DataFrame\n",
    "zone_df = spark.read.csv(\"taxi_zone_lookup.csv\", header=True, inferSchema=True)\n",
    "zone_df.createOrReplaceTempView(\"zones\")\n",
    "\n",
    "# Create or use a temporary view for the Yellow taxi data if not already done\n",
    "df.createOrReplaceTempView(\"yellow\")\n",
    "\n",
    "# Use a Spark SQL query to join the data, count trips per zone, and select the least frequent one.\n",
    "least_frequent_zone = spark.sql(\"\"\"\n",
    "SELECT z.zone, COUNT(*) as trip_count\n",
    "FROM yellow y\n",
    "JOIN zones z ON y.PULocationID = z.LocationID\n",
    "GROUP BY z.zone\n",
    "ORDER BY trip_count ASC\n",
    "\"\"\")\n",
    "#LIMIT 1\n",
    "\n",
    "least_frequent_zone.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62b30da-1db9-4d8a-9896-d48cf6c4317d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d50482-115a-447a-8578-c255fb385d0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b04599-3ab0-4e42-8766-4c6f659f0154",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python_3.10.12(.env_dataeng)",
   "language": "python",
   "name": ".env_dataeng"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
